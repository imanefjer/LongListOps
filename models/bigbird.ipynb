{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siVYM7RRw_3B"
      },
      "source": [
        "### README\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCGzqb75wclI"
      },
      "source": [
        "\n",
        "**Model Architecture and Training for ListOps using BigBird**\n",
        "\n",
        "**Overview**  \n",
        "This project fine-tunes a [BigBird](https://arxiv.org/abs/2007.14062) model on the ListOps dataset. The ListOps task involves evaluating hierarchical list expressions containing operations like `MIN`, `MAX`, `MED`, and `SM` over sequences of digits. Our goal is to predict the correct integer output (0-9) for each given expression.\n",
        "\n",
        "**Dataset**  \n",
        "The dataset is derived from the Long-Range-Arena (LRA) benchmark's ListOps task. It consists of expressions and their corresponding integer results. We use three splits:\n",
        "- **Training Set**\n",
        "- **Validation Set**\n",
        "- **Test Set**\n",
        "\n",
        "Each expression is tokenized into a custom vocabulary including special tokens (e.g. `[MIN`, `[MAX`, etc.), parentheses, and digits `0-9`. We pad these sequences to the maximum length in the batch and create an attention mask for the model.\n",
        "\n",
        "**Model**  \n",
        "We use a `BigBirdForSequenceClassification` model from the Hugging Face `transformers` library. BigBird employs sparse attention mechanisms with random and block-sparse attention patterns, allowing it to handle longer sequences efficiently compared to standard Transformers.\n",
        "\n",
        "- **Vocabulary Size:** Determined by the set of operators, digits, and special tokens in the dataset.\n",
        "- **Model Configuration:**\n",
        "  - **Hidden Size:** 256  \n",
        "  - **Number of Layers:** 4  \n",
        "  - **Number of Attention Heads:** 4  \n",
        "  - **Intermediate Size:** 1024  \n",
        "  - **Attention Type:** Block-sparse with random blocks  \n",
        "  - **Max Sequence Length:** Dynamically set to accommodate the longest expression in the dataset.\n",
        "\n",
        "These parameters can be adjusted depending on available compute resources and desired accuracy.\n",
        "\n",
        "**Training Procedure**  \n",
        "We train the model using the AdamW optimizer with a learning rate of `1e-4` for several epochs (e.g., 3). During training:\n",
        "- We feed batches of tokenized expressions and targets into the model.\n",
        "- The model outputs classification logits, and the loss is computed against the true target class.\n",
        "- We backpropagate the loss to update model parameters.\n",
        "\n",
        "We monitor training and validation loss and accuracy at the end of each epoch to ensure the model generalizes well.\n",
        "\n",
        "**Evaluation**  \n",
        "After training, we evaluate the model on the test set. The final accuracy on the test set gives an indication of how well the model has learned to solve the ListOps task.\n",
        "\n",
        "**Checkpoints**  \n",
        "Trained model weights are saved in the `model_checkpoints` directory. They can be reloaded later for further analysis or fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "**Running the Code**  \n",
        "1. Place the dataset files (`basic_train.tsv`, `basic_val.tsv`, `basic_test.tsv`) in the `data_dir` location specified in the script.\n",
        "2. Run the dataset loading script to generate `train_loader`, `val_loader`, and `test_loader`.\n",
        "3. Run the training script. If a GPU is available, the model will train on GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "693yGrdpwy7z"
      },
      "source": [
        "### code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_kUOi9bw2ll"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SSXNYJ5qqlI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing librairies"
      ],
      "metadata": {
        "id": "vrYhpr24zwM4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "991pXTkGBbsV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "sys.path.append(os.path.abspath('../data'))\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "import re\n",
        "import torch.optim as optim\n",
        "from torch.nn.functional import cross_entropy\n",
        "from transformers import BigBirdConfig, BigBirdForSequenceClassification\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset loading"
      ],
      "metadata": {
        "id": "pobdC_dTz0YI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dgBVBfXxBfgk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tokenize(expression):\n",
        "    \"\"\"Convert expression string to tokens, preserving operators.\"\"\"\n",
        "    # Replace parentheses with spaces\n",
        "    expr = expression.replace('(', ' ').replace(')', ' ')\n",
        "\n",
        "    # Add spaces around brackets that aren't part of operators\n",
        "    expr = re.sub(r'\\[(?!(MIN|MAX|MED|SM))', ' [ ', expr)\n",
        "    expr = expr.replace(']', ' ] ')\n",
        "\n",
        "    # Split and filter empty strings\n",
        "    return [token for token in expr.split() if token]\n",
        "\n",
        "class ListOpsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X: Array of source expressions\n",
        "            y: Array of target values\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        # Create vocabulary from operators and digits\n",
        "        self.vocab = {\n",
        "            'PAD': 0,  # Padding token\n",
        "            '[MIN': 1,\n",
        "            '[MAX': 2,\n",
        "            '[MED': 3,\n",
        "            '[SM': 4,\n",
        "            ']': 5,\n",
        "            '(': 6,\n",
        "            ')': 7\n",
        "        }\n",
        "        # Add digits 0-9\n",
        "        for i in range(10):\n",
        "            self.vocab[str(i)] = i + 8\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def tokenize(self, expr):\n",
        "        \"\"\"Convert expression to token IDs.\"\"\"\n",
        "        tokens = tokenize(expr)  # Using our previous tokenize function\n",
        "        return [self.vocab.get(token, 0) for token in tokens]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        expr = self.X[idx]\n",
        "        target = self.y[idx]\n",
        "\n",
        "        # Convert to token IDs without padding or truncating\n",
        "        token_ids = self.tokenize(expr)\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "            'target': torch.tensor(target, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "__aoAlH2Bfkx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t6Xzt7x0BfoE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WMm_38sAFAk",
        "outputId": "8b696854-d203-45e2-c62f-6ee0d660877d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n",
            "Loading validation data...\n",
            "Loading test data...\n",
            "\n",
            "Dataset sizes:\n",
            "Training: 96000 examples\n",
            "Validation: 2000 examples\n",
            "Test: 2000 examples\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the data directory and file paths\n",
        "data_dir = '/content/drive/MyDrive/LongListOps/data/output_dir'\n",
        "train_file = os.path.join(data_dir, 'basic_train.tsv')\n",
        "val_file = os.path.join(data_dir, 'basic_val.tsv')\n",
        "test_file = os.path.join(data_dir, 'basic_test.tsv')\n",
        "\n",
        "def load_listops_data(file_path, max_rows=None):\n",
        "    \"\"\"\n",
        "    Load ListOps data from TSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the TSV file\n",
        "        max_rows: Maximum number of rows to load (for testing)\n",
        "\n",
        "    Returns:\n",
        "        sources: Array of source expressions\n",
        "        targets: Array of target values (0-9)\n",
        "    \"\"\"\n",
        "    sources = []\n",
        "    targets = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        next(f)  # Skip header (Source, Target)\n",
        "        for i, line in enumerate(f):\n",
        "            if max_rows and i >= max_rows:\n",
        "                break\n",
        "            if not line.strip():  # Skip empty lines\n",
        "                continue\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) != 2:\n",
        "                continue  # Skip lines that don't have exactly two columns\n",
        "            source, target = parts\n",
        "            sources.append(source)\n",
        "            targets.append(int(target))  # Target is always 0-9\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    source_array = np.array(sources, dtype=object)  # Keep expressions as strings\n",
        "    target_array = np.array(targets, dtype=np.int32)  # Targets are integers\n",
        "\n",
        "    return source_array, target_array\n",
        "\n",
        "try:\n",
        "    # Load training data\n",
        "    print(\"Loading training data...\")\n",
        "    X_train, y_train = load_listops_data(train_file)\n",
        "\n",
        "    # Load validation data\n",
        "    print(\"Loading validation data...\")\n",
        "    X_val, y_val = load_listops_data(val_file)\n",
        "\n",
        "    # Load test data\n",
        "    print(\"Loading test data...\")\n",
        "    X_test, y_test = load_listops_data(test_file)\n",
        "\n",
        "    # Print dataset statistics\n",
        "    print(\"\\nDataset sizes:\")\n",
        "    print(f\"Training: {len(X_train)} examples\")\n",
        "    print(f\"Validation: {len(X_val)} examples\")\n",
        "    print(f\"Test: {len(X_test)} examples\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred: {type(e).__name__}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6GAqvfwEexi",
        "outputId": "838281af-2e52-4821-9638-ec870c66770e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset sizes:\n",
            "Train: 96000\n",
            "Val: 2000\n",
            "Test: 2000\n",
            "\n",
            "First batch shape:\n",
            "Input IDs: torch.Size([32, 1875])\n",
            "Targets: torch.Size([32])\n",
            "Sequence lengths: tensor([1875, 1778, 1722, 1612, 1455, 1445, 1407, 1345, 1218, 1168, 1000,  988,\n",
            "         974,  974,  944,  939,  927,  913,  886,  846,  828,  825,  739,  693,\n",
            "         692,  679,  644,  633,  612,  552,  526,  524])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def collate_fn(batch):\n",
        "    # Separate sequences and targets\n",
        "    sequences = [item['input_ids'] for item in batch]\n",
        "    targets = [item['target'] for item in batch]\n",
        "\n",
        "    # Get lengths of each sequence\n",
        "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long, device=sequences[0].device)\n",
        "\n",
        "    # Sort sequences by length in descending order for pack_padded_sequence\n",
        "    lengths, sort_idx = lengths.sort(descending=True)\n",
        "    sequences = [sequences[i] for i in sort_idx]\n",
        "    targets = [targets[i] for i in sort_idx]\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
        "\n",
        "    # Convert targets to tensor\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return {\n",
        "        'input_ids': padded_sequences,\n",
        "        'target': targets,\n",
        "        'lengths': lengths\n",
        "    }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ListOpsDataset(X_train, y_train)\n",
        "val_dataset = ListOpsDataset(X_val, y_val)\n",
        "test_dataset = ListOpsDataset(X_test, y_test)\n",
        "\n",
        "# Create dataloaders with collate_fn\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Verify the data\n",
        "print(\"Dataset sizes:\")\n",
        "print(f\"Train: {len(train_dataset)}\")\n",
        "print(f\"Val: {len(val_dataset)}\")\n",
        "print(f\"Test: {len(test_dataset)}\")\n",
        "\n",
        "# Check first batch\n",
        "batch = next(iter(train_loader))\n",
        "print(\"\\nFirst batch shape:\")\n",
        "print(f\"Input IDs: {batch['input_ids'].shape}\")\n",
        "print(f\"Targets: {batch['target'].shape}\")\n",
        "print(f\"Sequence lengths: {batch['lengths']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model definition and training"
      ],
      "metadata": {
        "id": "x75gMkYFz4ik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygkGTVaYvarb",
        "outputId": "e23fb5c4-da65-474b-dfd7-97178a67e4ca"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-602ab2669f08>:89: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/6000 [00:00<?, ?it/s]<ipython-input-5-602ab2669f08>:106: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training: 100%|██████████| 6000/6000 [06:20<00:00, 15.79it/s]\n",
            "<ipython-input-5-602ab2669f08>:135: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n",
            "Evaluating: 100%|██████████| 125/125 [00:02<00:00, 43.13it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7564, Train Acc: 0.3582\n",
            "Val Loss:   1.7261, Val Acc:   0.3600\n",
            "Epoch 2/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6000/6000 [06:14<00:00, 16.02it/s]\n",
            "Evaluating: 100%|██████████| 125/125 [00:02<00:00, 42.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7332, Train Acc: 0.3600\n",
            "Val Loss:   1.7284, Val Acc:   0.3650\n",
            "Epoch 3/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6000/6000 [06:13<00:00, 16.08it/s]\n",
            "Evaluating: 100%|██████████| 125/125 [00:03<00:00, 33.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7331, Train Acc: 0.3586\n",
            "Val Loss:   1.7246, Val Acc:   0.3630\n",
            "Epoch 4/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6000/6000 [06:20<00:00, 15.79it/s]\n",
            "Evaluating: 100%|██████████| 125/125 [00:02<00:00, 44.98it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7244, Train Acc: 0.3594\n",
            "Val Loss:   1.7175, Val Acc:   0.3560\n",
            "Epoch 5/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6000/6000 [06:14<00:00, 16.02it/s]\n",
            "Evaluating: 100%|██████████| 125/125 [00:03<00:00, 35.99it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.7114, Train Acc: 0.3609\n",
            "Val Loss:   1.7034, Val Acc:   0.3650\n",
            "Epoch 6/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6000/6000 [06:19<00:00, 15.82it/s]\n",
            "Evaluating: 100%|██████████| 125/125 [00:02<00:00, 44.45it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.6993, Train Acc: 0.3592\n",
            "Val Loss:   1.6918, Val Acc:   0.3650\n",
            "Epoch 7/8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 6000/6000 [06:13<00:00, 16.07it/s]\n",
            "Evaluating: 100%|██████████| 125/125 [00:02<00:00, 41.95it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.6929, Train Acc: 0.3599\n",
            "Val Loss:   1.6844, Val Acc:   0.3620\n",
            "Epoch 8/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 6000/6000 [06:06<00:00, 16.35it/s]\n",
            "Evaluating: 100%|██████████| 125/125 [00:02<00:00, 45.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6887, Train Acc: 0.3619\n",
            "Val Loss:   1.6820, Val Acc:   0.3615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 125/125 [00:03<00:00, 34.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.6577, Test Acc: 0.3700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "############################################\n",
        "# Preprocessing: Truncate Sequences to speed up training\n",
        "max_length = 512  # reduce this if original sequences are very long\n",
        "\n",
        "def truncate_batch(batch):\n",
        "    # Truncate sequences longer than max_length\n",
        "    batch_input = batch['input_ids']\n",
        "    if batch_input.size(1) > max_length:\n",
        "        batch['input_ids'] = batch_input[:, :max_length]\n",
        "    return batch\n",
        "\n",
        "############################################\n",
        "# Recreate DataLoaders with truncated sequences\n",
        "\n",
        "batch_size = 16  # Slightly larger than before, but still small enough for memory.\n",
        "\n",
        "def faster_collate_fn(batch):\n",
        "    # Same as collate_fn, but now we truncate after padding\n",
        "    sequences = [item['input_ids'] for item in batch]\n",
        "    targets = [item['target'] for item in batch]\n",
        "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long, device=sequences[0].device)\n",
        "\n",
        "    # Sort by length (descending)\n",
        "    lengths, sort_idx = lengths.sort(descending=True)\n",
        "    sequences = [sequences[i] for i in sort_idx]\n",
        "    targets = [targets[i] for i in sort_idx]\n",
        "\n",
        "    padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    out_batch = {\n",
        "        'input_ids': padded_sequences,\n",
        "        'target': targets,\n",
        "        'lengths': lengths\n",
        "    }\n",
        "\n",
        "    # Truncate here before returning\n",
        "    out_batch = truncate_batch(out_batch)\n",
        "    return out_batch\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=faster_collate_fn,\n",
        "    pin_memory=True,\n",
        "    num_workers=0\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=faster_collate_fn,\n",
        "    pin_memory=True,\n",
        "    num_workers=0\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=faster_collate_fn,\n",
        "    pin_memory=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "############################################\n",
        "# Configure a much smaller BigBird model\n",
        "\n",
        "vocab_size = len(train_dataset.vocab)\n",
        "\n",
        "config = BigBirdConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_size=64,            # Much smaller hidden size\n",
        "    num_hidden_layers=2,       # Fewer layers\n",
        "    num_attention_heads=2,     # Fewer heads\n",
        "    intermediate_size=256,     # Smaller intermediate size\n",
        "    max_position_embeddings=max_length,\n",
        "    num_labels=10,\n",
        "    attention_type=\"block_sparse\",\n",
        "    block_size=64,             # Larger block size, potentially fewer blocks to process\n",
        "    num_random_blocks=1        # Fewer random blocks reduces complexity\n",
        ")\n",
        "\n",
        "model = BigBirdForSequenceClassification(config)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "############################################\n",
        "# Training and Evaluation Functions\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        targets = batch['target'].to(device, non_blocking=True)\n",
        "        attention_mask = (input_ids != train_dataset.vocab['PAD']).long()\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        total_correct += (preds == targets).sum().item()\n",
        "        total_samples += input_ids.size(0)\n",
        "\n",
        "        # Free memory\n",
        "        del input_ids, targets, attention_mask, outputs, loss, logits, preds\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad(), autocast():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            targets = batch['target'].to(device, non_blocking=True)\n",
        "            attention_mask = (input_ids != train_dataset.vocab['PAD']).long()\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=targets)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            total_correct += (preds == targets).sum().item()\n",
        "            total_samples += input_ids.size(0)\n",
        "\n",
        "            del input_ids, targets, attention_mask, outputs, loss, logits, preds\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "############################################\n",
        "# Training Loop\n",
        "\n",
        "num_epochs = 8  # Try fewer epochs to check speed first.\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer)\n",
        "    val_loss, val_acc = evaluate(model, val_loader)\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss:   {val_loss:.4f}, Val Acc:   {val_acc:.4f}\")\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "output_dir = './model_checkpoints'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "model.save_pretrained(output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MLsaaJggvamT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jtUwexbAvadM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "siVYM7RRw_3B"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}