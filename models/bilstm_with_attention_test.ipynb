{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence,pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def tokenize(expression):\n",
    "    \"\"\"Convert expression string to tokens, preserving operators.\"\"\"\n",
    "    # Replace parentheses with spaces\n",
    "    expr = expression.replace('(', ' ').replace(')', ' ')\n",
    "\n",
    "    # Add spaces around brackets that aren't part of operators\n",
    "    expr = expr.replace(']', ' ] ')\n",
    "\n",
    "    # Split and filter empty strings\n",
    "    return [token for token in expr.split() if token]\n",
    "\n",
    "class ListOpsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Array of source expressions\n",
    "            y: Array of target values\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # Create vocabulary from operators and digits\n",
    "        self.vocab = {\n",
    "            'PAD': 0,  # Padding token\n",
    "            '[MIN': 1,\n",
    "            '[MAX': 2,\n",
    "            '[MED': 3,\n",
    "            '[SM': 4,\n",
    "            ']': 5,\n",
    "            '(': 6,\n",
    "            ')': 7\n",
    "        }\n",
    "        # Add digits 0-9\n",
    "        for i in range(10):\n",
    "            self.vocab[str(i)] = i + 8\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def tokenize(self, expr):\n",
    "        \"\"\"Convert expression to token IDs.\"\"\"\n",
    "        tokens = tokenize(expr)  # Using our previous tokenize function\n",
    "        return [self.vocab.get(token, 0) for token in tokens]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        expr = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "\n",
    "        # Convert to token IDs without padding or truncating\n",
    "        token_ids = self.tokenize(expr)\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "            'target': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading validation data...\n",
      "Loading test data...\n",
      "\n",
      "Dataset sizes:\n",
      "Training: 96000 examples\n",
      "Validation: 2000 examples\n",
      "Test: 2000 examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the data directory and file paths\n",
    "data_dir = '../data/output_dir'\n",
    "train_file = os.path.join(data_dir, 'basic_train.tsv')\n",
    "val_file = os.path.join(data_dir, 'basic_val.tsv')\n",
    "test_file = os.path.join(data_dir, 'basic_test.tsv')\n",
    "\n",
    "def load_listops_data(file_path, max_rows=None):\n",
    "    \"\"\"\n",
    "    Load ListOps data from TSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the TSV file\n",
    "        max_rows: Maximum number of rows to load (for testing)\n",
    "\n",
    "    Returns:\n",
    "        sources: Array of source expressions\n",
    "        targets: Array of target values (0-9)\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    targets = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # Skip header (Source, Target)\n",
    "        for i, line in enumerate(f):\n",
    "            if max_rows and i >= max_rows:\n",
    "                break\n",
    "            if not line.strip():  # Skip empty lines\n",
    "                continue\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue  # Skip lines that don't have exactly two columns\n",
    "            source, target = parts\n",
    "            sources.append(source)\n",
    "            targets.append(int(target))  # Target is always 0-9\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    source_array = np.array(sources, dtype=object)  # Keep expressions as strings\n",
    "    target_array = np.array(targets, dtype=np.int32)  # Targets are integers\n",
    "\n",
    "    return source_array, target_array\n",
    "\n",
    "try:\n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    X_train, y_train = load_listops_data(train_file)\n",
    "\n",
    "    # Load validation data\n",
    "    print(\"Loading validation data...\")\n",
    "    X_val, y_val = load_listops_data(val_file)\n",
    "\n",
    "    # Load test data\n",
    "    print(\"Loading test data...\")\n",
    "    X_test, y_test = load_listops_data(test_file)\n",
    "\n",
    "    # Print dataset statistics\n",
    "    print(\"\\nDataset sizes:\")\n",
    "    print(f\"Training: {len(X_train)} examples\")\n",
    "    print(f\"Validation: {len(X_val)} examples\")\n",
    "    print(f\"Test: {len(X_test)} examples\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {type(e).__name__}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate sequences and targets\n",
    "    sequences = [item['input_ids'] for item in batch]\n",
    "    targets = [item['target'] for item in batch]\n",
    "\n",
    "    # Get lengths of each sequence\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long, device=sequences[0].device)\n",
    "\n",
    "    # Sort sequences by length in descending order for pack_padded_sequence\n",
    "    lengths, sort_idx = lengths.sort(descending=True)\n",
    "    sequences = [sequences[i] for i in sort_idx]\n",
    "    targets = [targets[i] for i in sort_idx]\n",
    "\n",
    "    # Pad sequences\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
    "    # Convert targets to tensor\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return {\n",
    "        'input_ids': packed_sequences,\n",
    "        'target': targets,\n",
    "        'lengths': lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 96000\n",
      "Val: 2000\n",
      "Test: 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create datasets\n",
    "train_dataset = ListOpsDataset(X_train, y_train)\n",
    "val_dataset = ListOpsDataset(X_val, y_val)\n",
    "test_dataset = ListOpsDataset(X_test, y_test)\n",
    "\n",
    "# Create dataloaders with collate_fn\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Verify the data\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Val: {len(val_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Validation function\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(batch) \n",
    "            loss = criterion(outputs, batch['target'])\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += batch['target'].size(0)\n",
    "            correct += predicted.eq(batch['target']).sum().item()\n",
    "\n",
    "    return total_loss / len(val_loader), 100. * correct / total\n",
    "\n",
    "# Initialize model and training components\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.3, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "        # Final classification layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Get the packed sequence from the batch\n",
    "        packed_input = batch['input_ids']\n",
    "        \n",
    "        # Embed the input\n",
    "        embedded = self.embedding(packed_input.data)\n",
    "        packed_embedded = PackedSequence(embedded, packed_input.batch_sizes,\n",
    "                                       packed_input.sorted_indices, packed_input.unsorted_indices)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        packed_output, _ = self.lstm(packed_embedded)\n",
    "        \n",
    "        # Unpack the sequence\n",
    "        lstm_output, lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        batch_size = lstm_output.size(0)\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.attention(lstm_output)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Apply attention weights to get context vector\n",
    "        context_vector = torch.bmm(attention_weights.transpose(1, 2), lstm_output)\n",
    "        context_vector = context_vector.squeeze(1)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(context_vector)\n",
    "        return output\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "criterion = FocalLoss(gamma=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/myk783zx7hxfncdv93x9xn1h0000gn/T/ipykernel_62002/1799354043.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('lstm_with_attention_model.pth', map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.5770 | Test Acc: 62.25%\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(train_dataset.vocab)\n",
    "model = AttentionLSTM(vocab_size=vocab_size)\n",
    "model.load_state_dict(torch.load('lstm_with_attention_model.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
    "print(f'\\nTest Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lra_env_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
