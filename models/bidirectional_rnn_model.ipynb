{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0wSToy95GBnR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "sys.path.append(os.path.abspath('../data'))\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import PackedSequence, pack_padded_sequence, pad_packed_sequence,pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PyfvA0auLuXd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def tokenize(expression):\n",
        "    \"\"\"Convert expression string to tokens, preserving operators.\"\"\"\n",
        "    # Replace parentheses with spaces\n",
        "    expr = expression.replace('(', ' ').replace(')', ' ')\n",
        "\n",
        "    # Add spaces around brackets that aren't part of operators\n",
        "    expr = expr.replace(']', ' ] ')\n",
        "\n",
        "    # Split and filter empty strings\n",
        "    return [token for token in expr.split() if token]\n",
        "\n",
        "class ListOpsDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X: Array of source expressions\n",
        "            y: Array of target values\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "        # Create vocabulary from operators and digits\n",
        "        self.vocab = {\n",
        "            'PAD': 0,  # Padding token\n",
        "            '[MIN': 1,\n",
        "            '[MAX': 2,\n",
        "            '[MED': 3,\n",
        "            '[SM': 4,\n",
        "            ']': 5,\n",
        "            '(': 6,\n",
        "            ')': 7\n",
        "        }\n",
        "        # Add digits 0-9\n",
        "        for i in range(10):\n",
        "            self.vocab[str(i)] = i + 8\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def tokenize(self, expr):\n",
        "        \"\"\"Convert expression to token IDs.\"\"\"\n",
        "        tokens = tokenize(expr)  # Using our previous tokenize function\n",
        "        return [self.vocab.get(token, 0) for token in tokens]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        expr = self.X[idx]\n",
        "        target = self.y[idx]\n",
        "\n",
        "        # Convert to token IDs without padding or truncating\n",
        "        token_ids = self.tokenize(expr)\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "            'target': torch.tensor(target, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UubYVwfNGBnS",
        "outputId": "37813fc3-0788-4210-f13c-eb0e29c7ae93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "Loading validation data...\n",
            "Loading test data...\n",
            "\n",
            "Dataset sizes:\n",
            "Training: 96000 examples\n",
            "Validation: 2000 examples\n",
            "Test: 2000 examples\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the data directory and file paths\n",
        "data_dir = 'LongListOps/data/output_dir'\n",
        "train_file = os.path.join(data_dir, 'basic_train.tsv')\n",
        "val_file = os.path.join(data_dir, 'basic_val.tsv')\n",
        "test_file = os.path.join(data_dir, 'basic_test.tsv')\n",
        "\n",
        "def load_listops_data(file_path, max_rows=None):\n",
        "    \"\"\"\n",
        "    Load ListOps data from TSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the TSV file\n",
        "        max_rows: Maximum number of rows to load (for testing)\n",
        "\n",
        "    Returns:\n",
        "        sources: Array of source expressions\n",
        "        targets: Array of target values (0-9)\n",
        "    \"\"\"\n",
        "    sources = []\n",
        "    targets = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        next(f)  # Skip header (Source, Target)\n",
        "        for i, line in enumerate(f):\n",
        "            if max_rows and i >= max_rows:\n",
        "                break\n",
        "            if not line.strip():  # Skip empty lines\n",
        "                continue\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) != 2:\n",
        "                continue  # Skip lines that don't have exactly two columns\n",
        "            source, target = parts\n",
        "            sources.append(source)\n",
        "            targets.append(int(target))  # Target is always 0-9\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    source_array = np.array(sources, dtype=object)  # Keep expressions as strings\n",
        "    target_array = np.array(targets, dtype=np.int32)  # Targets are integers\n",
        "\n",
        "    return source_array, target_array\n",
        "\n",
        "try:\n",
        "    # Load training data\n",
        "    print(\"Loading training data...\")\n",
        "    X_train, y_train = load_listops_data(train_file)\n",
        "\n",
        "    # Load validation data\n",
        "    print(\"Loading validation data...\")\n",
        "    X_val, y_val = load_listops_data(val_file)\n",
        "\n",
        "    # Load test data\n",
        "    print(\"Loading test data...\")\n",
        "    X_test, y_test = load_listops_data(test_file)\n",
        "\n",
        "    # Print dataset statistics\n",
        "    print(\"\\nDataset sizes:\")\n",
        "    print(f\"Training: {len(X_train)} examples\")\n",
        "    print(f\"Validation: {len(X_val)} examples\")\n",
        "    print(f\"Test: {len(X_test)} examples\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred: {type(e).__name__}: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kxcaqX5i6Zpz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def collate_fn(batch):\n",
        "    # Separate sequences and targets\n",
        "    sequences = [item['input_ids'] for item in batch]\n",
        "    targets = [item['target'] for item in batch]\n",
        "\n",
        "    # Get lengths of each sequence\n",
        "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long, device=sequences[0].device)\n",
        "\n",
        "    # Sort sequences by length in descending order for pack_padded_sequence\n",
        "    lengths, sort_idx = lengths.sort(descending=True)\n",
        "    sequences = [sequences[i] for i in sort_idx]\n",
        "    targets = [targets[i] for i in sort_idx]\n",
        "\n",
        "    # Pad sequences\n",
        "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
        "    packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
        "    # Convert targets to tensor\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return {\n",
        "        'input_ids': packed_sequences,\n",
        "        'target': targets,\n",
        "        'lengths': lengths\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXgSNm7GGBnS",
        "outputId": "3022824e-c91b-47cf-dd11-daadd5e4cb4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sizes:\n",
            "Train: 96000\n",
            "Val: 2000\n",
            "Test: 2000\n",
            "\n",
            "First batch shape:\n",
            "Input IDs: torch.Size([32, 1815])\n",
            "Targets: torch.Size([32])\n",
            "Sequence lengths: tensor([1815, 1712, 1682, 1658, 1643, 1638, 1525, 1476, 1443, 1330, 1325, 1193,\n",
            "        1191, 1187, 1096, 1045, 1042,  945,  892,  870,  779,  773,  765,  764,\n",
            "         723,  641,  635,  621,  608,  605,  560,  506])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create datasets\n",
        "train_dataset = ListOpsDataset(X_train, y_train)\n",
        "val_dataset = ListOpsDataset(X_val, y_val)\n",
        "test_dataset = ListOpsDataset(X_test, y_test)\n",
        "\n",
        "# Create dataloaders with collate_fn\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Verify the data\n",
        "print(\"Dataset sizes:\")\n",
        "print(f\"Train: {len(train_dataset)}\")\n",
        "print(f\"Val: {len(val_dataset)}\")\n",
        "print(f\"Test: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvPhWw8UGBnT",
        "outputId": "1f8ae764-b719-4914-a581-2acb108182bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Validation function\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Move batch to device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            outputs = model(batch)  # Pass the entire batch dictionary\n",
        "            loss = criterion(outputs, batch['target'])\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += batch['target'].size(0)\n",
        "            correct += predicted.eq(batch['target']).sum().item()\n",
        "\n",
        "    return total_loss / len(val_loader), 100. * correct / total\n",
        "\n",
        "# Initialize model and training components\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HFa_X3T2NTFp"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = self.ce(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss.mean()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nx1BkC-voBsm"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = model.state_dict()\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model = model.state_dict()\n",
        "            self.counter = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B98UKQ5DoEp7",
        "outputId": "862ea93b-cd9b-4407-c0fe-834d508ad14e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 750/750 [03:45<00:00,  3.32it/s, loss=0.0093, acc=37.50%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/10\n",
            "Train Loss: 1.1878 | Train Acc: 37.50%\n",
            "Val Loss: 1.1264 | Val Acc: 39.25%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 750/750 [03:45<00:00,  3.32it/s, loss=0.0089, acc=39.03%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2/10\n",
            "Train Loss: 1.1379 | Train Acc: 39.03%\n",
            "Val Loss: 1.0999 | Val Acc: 40.45%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 750/750 [03:45<00:00,  3.32it/s, loss=0.0088, acc=39.72%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3/10\n",
            "Train Loss: 1.1230 | Train Acc: 39.72%\n",
            "Val Loss: 1.1048 | Val Acc: 39.20%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 750/750 [03:46<00:00,  3.31it/s, loss=0.0087, acc=40.09%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4/10\n",
            "Train Loss: 1.1145 | Train Acc: 40.09%\n",
            "Val Loss: 1.0778 | Val Acc: 40.75%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 750/750 [03:53<00:00,  3.21it/s, loss=0.0086, acc=40.53%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5/10\n",
            "Train Loss: 1.1063 | Train Acc: 40.53%\n",
            "Val Loss: 1.0945 | Val Acc: 40.80%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 750/750 [03:52<00:00,  3.23it/s, loss=0.0086, acc=40.69%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6/10\n",
            "Train Loss: 1.1039 | Train Acc: 40.69%\n",
            "Val Loss: 1.0836 | Val Acc: 40.55%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 750/750 [03:53<00:00,  3.21it/s, loss=0.0086, acc=41.01%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7/10\n",
            "Train Loss: 1.0997 | Train Acc: 41.01%\n",
            "Val Loss: 1.0849 | Val Acc: 39.90%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 750/750 [03:47<00:00,  3.30it/s, loss=0.0086, acc=40.93%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8/10\n",
            "Train Loss: 1.0978 | Train Acc: 40.93%\n",
            "Val Loss: 1.0961 | Val Acc: 40.15%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 750/750 [03:45<00:00,  3.32it/s, loss=0.0083, acc=42.40%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9/10\n",
            "Train Loss: 1.0679 | Train Acc: 42.40%\n",
            "Val Loss: 1.0668 | Val Acc: 41.55%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 750/750 [03:47<00:00,  3.29it/s, loss=0.0083, acc=43.02%]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10/10\n",
            "Train Loss: 1.0592 | Train Acc: 43.02%\n",
            "Val Loss: 1.0480 | Val Acc: 42.50%\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class SimpleRNNModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size=18,\n",
        "                 embedding_dim=128,\n",
        "                 hidden_dim=256,\n",
        "                 num_layers=2,\n",
        "                 dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 10)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        input_ids = batch['input_ids']\n",
        "        lengths = batch['lengths']\n",
        "\n",
        "        # Handle embedding layer\n",
        "        if isinstance(input_ids, PackedSequence):\n",
        "            # If input is already packed, unpack it first\n",
        "            seq, lens = pad_packed_sequence(input_ids, batch_first=True)\n",
        "            x = self.embedding(seq)\n",
        "            x = self.dropout(x)\n",
        "            x = pack_padded_sequence(x, lens, batch_first=True, enforce_sorted=False)\n",
        "        else:\n",
        "            # Regular sequence processing\n",
        "            x = self.embedding(input_ids)\n",
        "            x = self.dropout(x)\n",
        "            x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Process with RNN\n",
        "        packed_output, hidden = self.rnn(x)\n",
        "\n",
        "        # Concatenate the last hidden states from both directions\n",
        "        hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        x = self.dropout(hidden_cat)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "# Training setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpleRNNModel().to(device)\n",
        "criterion = FocalLoss(gamma=2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "early_stopping = EarlyStopping(patience=5)\n",
        "\n",
        "# Dataloaders (make sure to use the collate_fn defined earlier)\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                         num_workers=4, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                       num_workers=4, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                        num_workers=4, collate_fn=collate_fn)\n",
        "num_epochs = 10\n",
        "print(\"\\nStarting training...\")\n",
        "# First, update the training loop to pass the batch as a dictionary\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "    for batch in pbar:\n",
        "        # Move all batch tensors to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch)  # Pass the entire batch dictionary\n",
        "        loss = criterion(outputs, batch['target'])\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += batch['target'].size(0)\n",
        "        correct += predicted.eq(batch['target']).sum().item()\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{train_loss/total:.4f}',\n",
        "                         'acc': f'{100.*correct/total:.2f}%'})\n",
        "\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Epoch: {epoch+1}/{num_epochs}')\n",
        "    print(f'Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {100.*correct/total:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n",
        "    print('-' * 50)\n",
        "\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        model.load_state_dict(early_stopping.best_model)\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSDSzD0Oe0xI",
        "outputId": "fe7b5c70-8d1d-435f-8c4e-c16792bc3f9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss: 1.0222 | Test Acc: 45.20%\n"
          ]
        }
      ],
      "source": [
        "# Test with best model\n",
        "test_loss, test_acc = validate(model, test_loader, criterion, device)\n",
        "print(f'\\nTest Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9iYDRTuL8iMc"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'rnn_model.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
